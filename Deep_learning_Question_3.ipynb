{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5NxcmEdC6izxy9YHyIlwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athulsunny/Ineuron_Interview_Assessment/blob/main/Deep_learning_Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Pure CNN with less than 10000 trainable parameters using the MNIST\n",
        "Dataset having minimum validation accuracy of 99.40%\n",
        "Note -\n",
        "1. Code comments should be given for proper code understanding.\n",
        "2. Implement in both PyTorch and Tensorflow respectively"
      ],
      "metadata": {
        "id": "-M2QVnx3c0b0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Implementation:"
      ],
      "metadata": {
        "id": "v1vC8aAfeEdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "Khu8E915c4oL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "O1alhmiYc4q6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "train_dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnLaYLbTc4te",
        "outputId": "78938964-2a9a-46d4-e236-5bcda17c6bce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 107826420.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 57766186.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 23568251.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11928947.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EBs55lzCc4v6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = CNNModel().to(device)\n",
        "\n",
        "# Define the optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to train the model\n",
        "def train(model, optimizer, criterion, train_loader):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "HlYVIP0Rc4ye"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate(model, criterion, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return (100 * correct / total)"
      ],
      "metadata": {
        "id": "S9m5d1bfc41E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 20\n",
        "min_val_accuracy = 99.40  # Minimum validation accuracy threshold\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train(model, optimizer, criterion, train_loader)\n",
        "    val_accuracy = evaluate(model, criterion, test_loader)\n",
        "    \n",
        "    if val_accuracy >= min_val_accuracy:\n",
        "        break\n",
        "\n",
        "# Print the final accuracy\n",
        "print(\"Validation Accuracy: {:.2f}%\".format(val_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSybNpxrc43o",
        "outputId": "36f5d02c-9fb2-4ac2-ce42-d94e5031fdc0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow Implementation:\n"
      ],
      "metadata": {
        "id": "WwG8LOjIexkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "HhQQnPKuc48c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JfxI9Mqc4_C",
        "outputId": "1ef2da08-cdb0-4779-9e90-9abc54899061"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize and reshape the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)"
      ],
      "metadata": {
        "id": "MB_rB2ECc5BR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "7O7kUUNtc5hJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v9w8wgLc5jX",
        "outputId": "314fab9f-303c-4285-e6b4-a3d300100c1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1568)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               200832    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,922\n",
            "Trainable params: 206,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yar1mDkVkH00",
        "outputId": "c737ba2d-a96a-4b25-9090-80edc91d517c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "min_val_accuracy = 99.40  # Minimum validation accuracy threshold\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1)\n",
        "    val_loss, val_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    \n",
        "    if val_accuracy >= min_val_accuracy:\n",
        "        break\n",
        "\n",
        "# Print the final accuracy\n",
        "print(\"Validation Accuracy: {:.2f}%\".format(val_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7n7Cs4FkH3L",
        "outputId": "d7e63704-51a5-4b96-dd1d-1d8d6d1db517"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 47s 98ms/step - loss: 0.2465 - accuracy: 0.9293\n",
            "469/469 [==============================] - 40s 84ms/step - loss: 0.0684 - accuracy: 0.9794\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0472 - accuracy: 0.9857\n",
            "469/469 [==============================] - 41s 87ms/step - loss: 0.0359 - accuracy: 0.9889\n",
            "469/469 [==============================] - 42s 90ms/step - loss: 0.0297 - accuracy: 0.9907\n",
            "469/469 [==============================] - 40s 85ms/step - loss: 0.0236 - accuracy: 0.9924\n",
            "469/469 [==============================] - 40s 85ms/step - loss: 0.0185 - accuracy: 0.9941\n",
            "469/469 [==============================] - 39s 82ms/step - loss: 0.0157 - accuracy: 0.9949\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0138 - accuracy: 0.9957\n",
            "469/469 [==============================] - 40s 85ms/step - loss: 0.0112 - accuracy: 0.9962\n",
            "469/469 [==============================] - 40s 86ms/step - loss: 0.0090 - accuracy: 0.9971\n",
            "469/469 [==============================] - 40s 84ms/step - loss: 0.0087 - accuracy: 0.9971\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0065 - accuracy: 0.9980\n",
            "469/469 [==============================] - 41s 87ms/step - loss: 0.0069 - accuracy: 0.9977\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0047 - accuracy: 0.9985\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0055 - accuracy: 0.9982\n",
            "469/469 [==============================] - 43s 92ms/step - loss: 0.0047 - accuracy: 0.9982\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0049 - accuracy: 0.9983\n",
            "469/469 [==============================] - 40s 85ms/step - loss: 0.0040 - accuracy: 0.9987\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0042 - accuracy: 0.9986\n",
            "Validation Accuracy: 99.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbi06nfSkH7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKAAVvh2kH9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}